---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
# %autosave 0
```

ideas:
1. data processing - one hot encoding, normalization
2. quick EDA
3. set up cv5
4. get a baseline performance by using the data as it is with a few models (lr, svm, random forest, lda)
5. feature engineering and re-train


# Data import and cleaning

```{python}
train_path = "data/train.csv"
test_path = "data/test.csv"
```

```{python}
import numpy as np
import pandas as pd
```

```{python}
import matplotlib.pyplot as plt
```

```{python}
train_df = pd.read_csv(train_path)
```

```{python}
test_df = pd.read_csv(test_path)
```

```{python}
train_df.head()
```

```{python}
train_df.describe()
```

```{python}
train_df.info()
```

```{python}
train_df['outcome'].value_counts()
```

```{python}
train_df.hist(figsize=(12,8))
plt.show()
```

# Data preprocessing

```{python}
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
```

```{python}
cat_features = ['device_type', 'gender', 'in_initial_launch_location']

def data_preprocessing(data, use_scale=False, scale=None, onehot=True):
    data['gender'].fillna(data['gender'].mode().iloc[0], inplace=True)
    
    if onehot: 
        one_hot = OneHotEncoder(sparse=False)
        one_hot.fit(data['in_initial_launch_location'].values.reshape(-1,1))
        location_onehot = one_hot.fit_transform(data['in_initial_launch_location'].values.reshape(-1,1))
        data = data.join(pd.DataFrame(location_onehot, columns=['location_onehot1','location_onehot2'])).drop('in_initial_launch_location',axis=1)

        one_hot = OneHotEncoder(sparse=False)
        device_onehot = one_hot.fit_transform(data['device_type'].values.reshape(-1,1))
        data = data.join(pd.DataFrame(device_onehot, columns=['device_onehot1','device_onehot2','device_onehot3','device_onehot4','device_onehot5'])).drop('device_type',axis=1)

        one_hot = OneHotEncoder(sparse=False, handle_unknown='ignore')
        gender_onehot = one_hot.fit_transform(data['gender'].values.reshape(-1,1))
        data = data.join(pd.DataFrame(gender_onehot, columns=['gender_onehot1','gender_onehot2'])).drop('gender',axis=1)
        
    else:
        ord_encoder = OrdinalEncoder()
        ords = pd.DataFrame(ord_encoder.fit_transform(train_df[cat_features]), columns = cat_features)
        data[cat_features] = ords
        
    if use_scale and scale!=None:
        data = pd.DataFrame(scale.fit_transform(data.iloc[:,0:6]), columns = data.iloc[:,0:6].columns).join(data.iloc[:,6:])

    return data
```

```{python}
train = data_preprocessing(train_df, use_scale=True, scale=MinMaxScaler())
train.head()
```

```{python}
train = data_preprocessing(train_df, use_scale=True, scale=MinMaxScaler(), onehot=False)
train.head()
```

```{python}
train.hist(figsize=(12,8))
plt.show()
```

# Model comparison

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier
```

```{python}
X = train.drop('outcome', axis=1)
y = train['outcome']
```

```{python}
#X.drop(["device_onehot1","device_onehot2","device_onehot3","device_onehot4",'device_onehot5'], axis=1, inplace=True)
```

```{python}
model_dict = {'lr': LogisticRegression(max_iter=200),
              'svm': SVC(), 
              'cart': DecisionTreeClassifier(), 
              'rf': RandomForestClassifier(),
              'ldr': LinearDiscriminantAnalysis()
             }
```

```{python}
cv_scores = []
for i, model in enumerate(model_dict):
    cv_score = cross_val_score(model_dict[model],X, y, scoring='roc_auc', cv=5)
    cv_scores.append(cv_score)
    
cv_scores = pd.DataFrame(cv_scores, index=list(model_dict.keys()))
cv_scores.mean()
```

```{python}
cv_score = cross_val_score(LogisticRegression(max_iter=200), X, y, scoring='roc_auc', cv=5)
print(f"scores:{cv_score}, \n mean:{cv_score.mean()}")
```

```{python}
cv_score = cross_val_score(SVC(),X, y, scoring='roc_auc', cv=5)
print(f"scores:{cv_score}, \n mean:{cv_score.mean()}")
```

```{python}
cv_score = cross_val_score(DecisionTreeClassifier(), X, y, scoring='roc_auc', cv=5)
print(f"scores:{cv_score}, \n mean:{cv_score.mean()}")
```

```{python}
cv_score = cross_val_score(RandomForestClassifier(n_estimators=500, random_state=42),X, y, scoring='roc_auc', cv=5)
print(f"scores:{cv_score}, \n mean:{cv_score.mean()}")
```

```{python}
cv_score = cross_val_score(LinearDiscriminantAnalysis(), X, y, scoring='roc_auc', cv=5)
print(f"scores:{cv_score}, \n mean:{cv_score.mean()}")
```

```{python}
cv_score = cross_val_score(XGBClassifier(n_estimators=500, random_state=42), X, y, scoring='roc_auc', cv=5)
print(f"scores:{cv_score}, \n mean:{cv_score.mean()}")
```

**observation**: 
- lr is sensitive to feature scaling, rf is not so much
- minmax scaling is better
- onehot encoding gives better results


# Feature engineering

```{python}
train = data_preprocessing(train_df, use_scale=False, scale=MinMaxScaler())
train['cost/driver'] = train['cost_of_ad']/train['n_drivers']
train['cost/vehicle'] = train['cost_of_ad']/train['n_vehicles']
train['income/driver'] = train['income']/train['n_drivers']
train['income/vehicle'] = train['income']/train['n_vehicles']
train['vehicle/driver'] = train['n_vehicles']/train['n_drivers']
train.head()
```

```{python}
num_features = [0,1,2,3,4,5,-5,-4,-3,-2,-1]
scale = MinMaxScaler()
train.iloc[:, num_features] = pd.DataFrame(scale.fit_transform(train.iloc[:,num_features]), columns = train.iloc[:,num_features].columns)
train.head()
```

```{python}
X = train.drop('outcome', axis=1)
y = train['outcome']
```

```{python}
cv_score = cross_val_score(LogisticRegression(),X, y, scoring='roc_auc', cv=5)
print(f"scores:{cv_score}, \n mean:{cv_score.mean()}")
```

```{python}
cv_score = cross_val_score(RandomForestClassifier(n_estimators=500, random_state=42),X, y, scoring='roc_auc', cv=5)
print(f"scores:{cv_score}, \n mean:{cv_score.mean()}")
```

```{python}
cv_score = cross_val_score(LinearDiscriminantAnalysis(),X, y, scoring='roc_auc', cv=5)
print(f"scores:{cv_score}, \n mean:{cv_score.mean()}")
```

```{python}
cv_score = cross_val_score(XGBClassifier(n_estimators=14, learning_rate=0.3), X, y, scoring='roc_auc', cv=5)
print(f"scores:{cv_score}, \n mean:{cv_score.mean()}")
```

**observation**: 
- the several engieered features are effective to lr


# Train test split training

```{python}
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)
for train_index, test_index in split.split(X, y):
    strat_train_set = train.loc[train_index]
    strat_test_set = train.loc[test_index]
```

```{python}
X_train = strat_train_set.drop('outcome', axis=1)
y_train = strat_train_set['outcome']
X_test = strat_test_set.drop('outcome', axis=1)
y_test = strat_test_set['outcome']
```

```{python}
from sklearn import metrics
```

```{python}
def train_model(model, x_train, y_train, x_test, y_test):
    model.fit(x_train, y_train)
    
    y_pred = model.predict(x_test)
    y_scores = model.predict_proba(x_test)

    return model, metrics.roc_auc_score(y_test, y_scores[:,1])
```

```{python}
_, auc = train_model(LogisticRegression(), X_train, y_train, X_test, y_test)
print(auc)
```

```{python}
_, auc = train_model(SVC(probability=True), X_train, y_train, X_test, y_test)
print(auc)
```

```{python}
_, auc = train_model(DecisionTreeClassifier(), X_train, y_train, X_test, y_test)
print(auc)
```

```{python}
rf, auc = train_model(RandomForestClassifier(n_estimators=500, random_state=42), X_train, y_train, X_test, y_test)
print(auc)
```

```{python}
pd.DataFrame(rf.feature_importances_, index = train.drop('outcome', axis=1).columns).sort_values(by=[0], ascending=False)
```

```{python}
_, auc = train_model(LinearDiscriminantAnalysis(), X_train, y_train, X_test, y_test)
print(auc)
```

```{python}
xg, auc = train_model(XGBClassifier(n_estimators=14, learning_rate=0.3), X_train, y_train, X_test, y_test)
print(auc)
```

```{python}
pd.DataFrame(xg.feature_importances_, index = train.drop('outcome', axis=1).columns).sort_values(by=[0], ascending=False)
```

```{python}
my_model = XGBClassifier(n_estimators=15, learning_rate=0.3)

my_model.fit(X_train, y_train,
             early_stopping_rounds=5,
            eval_set=[(X_test, y_test)])

y_scores = my_model.predict_proba(X_test)
metrics.roc_auc_score(y_test, y_scores[:,1])
```

```{python}
def zero_classifier(X):
    return np.zeros(len(X))
```

```{python}
zero_classifier(X_test)
```

```{python}
metrics.roc_auc_score(y_test, zero_classifier(X_test))
```

```{python}
from sklearn.model_selection import cross_val_predict
```

```{python}
from sklearn.metrics import roc_curve

model_dict = {'lr': LogisticRegression(),
              'svm': SVC(probability=True), 
              'cart': DecisionTreeClassifier(), 
              'rf': RandomForestClassifier(),
              'ldr': LinearDiscriminantAnalysis(),
              'xgb': XGBClassifier(n_estimators=14, learning_rate=0.3)
             }

fprs = []
tprs = []

for i, model in enumerate(model_dict):
    y_scores = cross_val_predict(model_dict[model], X_train, y_train, cv=3, method="predict_proba")[:, 1]
    fpr, tpr, thresholds = roc_curve(y_train, y_scores)
    fprs.append(fpr)
    tprs.append(tpr)

```

```{python}
def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.axis([0, 1, 0, 1])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')

for i, model in enumerate(model_dict):
    plot_roc_curve(fprs[i], tprs[i], model)
    
plt.legend()
plt.show()

```

# Grid search

```{python}
from sklearn.model_selection import GridSearchCV
```

```{python}
RandomForestClassifier()
```

```{python}
param_grid = [{'n_estimators': [50, 100, 200], 'max_features': np.arange(3,16,3)},
              
             ]
rf = RandomForestClassifier()
grid_search = GridSearchCV(rf, param_grid, cv=5,
                           scoring='roc_auc', verbose=2, n_jobs=8)
grid_search.fit(X, y)

```

```{python}
grid_search.best_params_
```

```{python}
grid_search.best_estimator_
```

```{python}
cvres = grid_search.cv_results_
```

```{python}
for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):
    print(mean_score, params)
```

```{python}

```

```{python}

```

```{python}
test_df.info()
```

```{python}

```
